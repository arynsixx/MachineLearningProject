# MachineLearningProject
 A project that use the CoreML model to recognize the emotions of an audio.
 
 This project is based on recognizing the emotions elicited by the interlocutor while speaking. 
 To do this, I trained machine learning using the CreateML framework, the model has been trained to classify seven different emotions: neutral, happy, sad, angry, fearful, disgust and surprised.
 
 The dataset was constructed using 4801 samples from: 
 - Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) dataset.
 - Toronto emotional speech set (TESS) dataset.
 
This Artcicle on Medium explains more about how the project was created:
